---
title: "carrerfour_analysis"
author: "Bill"
date: "11/13/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# 1. Problem Definition

## 1.1 Defining the Question

As a Data analyst at Carrefour Kenya,I have been consulted to undertake a project that will inform the marketing department on the most relevant marketing strategies that will result in the highest no. of sales (total price including tax). My  project will explore a recent marketing dataset provided by performing various unsupervised learning techniques and later providing recommendations based on my insights.

## 1.2 Specifying the Question
 I am expected to reduce the  dataset provided to a low dimensional dataset using the t-SNE algorithm or PCA, then perform  analysis and provide insights gained from my  analysis.

## 1.3  Defining the Metric of Success
- Reducing the dataset using PCA and t-SNE and getting the most effective/useful features
- Analysis that will enable the marketing department increase their total sales

## 1.4 Understanding the Context

Carrefour, one of the largest hypermarket chains in the world was introduced to the Middle East and North Africa (MENA) market in 1995 by Majid Al Futtaim, the leading shopping mall, retail and leisure pioneer across MENA.

Carrefour has become the most dynamic, fast-moving and exciting hypermarket chain in the region and shared its growth with more than 38,000 employees from more than 70 nationalities in 15 countries, providing shoppers with variety and value-for-money. 

Carrefour ensures customer satisfaction and everyday convenience while offering unbeatable value for money with a vast array of more than 100,000 products, shoppers can purchase items for their every need, whether home electronics or fresh fruits from around the world, to locally produced items. 

Carrefour opened its first outlet in Kenya in 2016, and currently operates over 250 hypermarkets, supermarkets, and online stores in 15 countries across the region, with plans to extend into 38 countries in the Middle East, Central Asia, Africa and Russia.

Carrefour always strive to provide the best quality and most diverse selection of household goods available in Kenya. Our value packs and combination discount offers means that we can offer these products at even lower costs, keeping your household essentials at unbeatable prices.


## 1.5 Experimental Design taken
1. Data Exploration
2. Data Cleaning and Formatting
3. Univariate Analysis
4. Bivariate Analysis
5. Multivariate Analysis
6. Dimensionality reduction using pca
6. Conclusion and Next steps
# 2. Data Sourcing
The data was availed to our data science team by the  Carrefourâ€™s Sales and Marketing team therefore no data collection and scrapping was needed...We will just load our dataset in RStudio and begin the analysis process
# 3. Check the Data
``` {r}
## Loading packages that we will use during our analysis
library("dplyr")
library("purrr")
library('tidyverse')
library('magrittr')
library('corrplot')
library('caret')
library('skimr')
library(readr)

```

``` {r}
Supermarket_Dataset<- read_csv("datasets/Supermarket_Dataset_1 - Sales Data.csv")

## previewing first 6 rows
head(Supermarket_Dataset)

##previewing the last 6 rows of the dataset
tail(Supermarket_Dataset)
```


``` {r}
## Previewing the shape of our dataset
dim(Supermarket_Dataset)
### we have 1000  rows and 16 columns!!!BAM!!

#checking the datatypes on the columns 
sapply(Supermarket_Dataset, class)
### Our dataset is comprised of numeric and character data types and one datetime
##checking for structure is using the str()
str(Supermarket_Dataset)

## We then a statistical summary of our dataset

summary(Supermarket_Dataset)

```


## Appropriateness of Our Dataset

The dataset contains 1000  rows and 16 columns which is sufficient enough for perfoming our analysis..We have 16 features which we will be forced to reduce them to  a small number using PCA and t-SNE


# 4. Perform Data Cleaning

## To ensure uniformity, I will lowercase all the columns
``` {r}
names(Supermarket_Dataset)<- tolower(names(Supermarket_Dataset))
head(Supermarket_Dataset) 


```

NEXT I'm going to checking for missing values in our dataset,,Missing values may affect the perfomance of our model, so we will find a way to deal with them

``` {r}
##Checking for missing values in each row
colSums(is.na(Supermarket_Dataset))

```

There are no missing values in our dataset...The Company did a pretty good job in data collection and data entry

So far so good!!! Lets now check for duplicates in our dataset,,,This may arise due to mistakes incurred during data collection and data entry.. R gives us better ways of dealing and checking duplicates,,,Lets explore them below

``` {r}
anyDuplicated(Supermarket_Dataset)

```
We also dont have duplicates in our dataset,,,,Nice

Next we are going to check for outliers in our numerical data,,This can be very extremely high or low values in our dataset that needs investigation...We only check for outliers in our numerical columns

``` {r}
## obtaining numerical columns
numeric_columns <- unlist(lapply(Supermarket_Dataset, is.numeric))

numeric_columns
## I will put the numerical columns in a dataframe

columns_numeric <- Supermarket_Dataset[ , numeric_columns]

head(columns_numeric)

```

```{r}
# using a for lop, I will output boxplots of numerical columns..This will help me to identify the outliers

par ( mfrow= c (  2, 4 ))
for (i in 1 : length (columns_numeric)) {
boxplot (columns_numeric[,i], main= names (columns_numeric[i]), type= "l" )
}
```

From the boxplots, outliers are present in the tax, cogs, gross income and totals columns....Dropping this outliers may affect our analysis as the extreme values might be genuine records..So I will not drop any outliers


Next We will check anomalies and inconsistenicies in our dataframe....Since we have categorical column, we will need to convert them to factors


``` {r}
lengths(lapply(Supermarket_Dataset, unique))


```

Our values seems fine......There is no need for an alarm

## Univariate Analysis
Here we analyse single variables by checking the measures of central tendency and measures of dispersion...
Lets begin with measures of central tendency 

``` {r}
## Getting the mean of all numerical columns
colMeans(columns_numeric)

```
##Median
``` {r}
apply(columns_numeric,2,median)

```

### Mode

``` {r}

getmode <- function(v) {
   uniqv <- unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]
}

##getting the mode of all our numerical columns
apply(columns_numeric,2,getmode)

```

##Measures of Dispersion

``` {r}
## we will do this by finding the statistical summary
summary(Supermarket_Dataset)


```

## Standard Deviation
``` {r}
apply(columns_numeric,2,sd)

```

## Variance
``` {r}

sapply(columns_numeric, var)

```


## Bivariate and Multivariate Analysis
I Will check for the corelation of all our numerical columns

##Correlation matrix of all numerical columns
``` {r}

corr <- cor(columns_numeric, method = "pearson")

round(corr, 2)

```

## Observations

Unit price has a strong positive correlation with total of 0.63
quantity has a strong positive correlation with total of 0.71    
tax, gross income and cogs are strongly correlated to total with a correlation of 1

## Implementing the Solution
## PCA

Principal component analysis is a widely used and popular statistical method for reducing data with many dimensions (variables) by projecting the data with fewer dimensions using linear combinations of the variables, known as principal components. 

``` {r}
head(columns_numeric)

df<- columns_numeric[,c(1,2,3,4,6,7,8)]
head(df)

```

``` {r}
# We then pass df to the prcomp(). We also set two arguments, center and scale, 
# to be TRUE then preview our object with summary
# ---
# 
supermarket.pca <- prcomp(df, center = TRUE, scale. = TRUE)

summary(supermarket.pca)

# As a result we obtain 7 principal components, 
# each which explain a percentate of the total variation of the dataset
# PC1 explains 70% of the total variance, which means that nearly two-thirds 
# of the information in the dataset (7 variables) can be encapsulated 
# by just that one Principal Component. PC2 explains 23% of the variance. etc
```

``` {r}
# Calling str() to have a look at your PCA object
# ---
# 
str(supermarket.pca)


# Here we note that our pca object: The center point ($center), scaling ($scale), 
# standard deviation(sdev) of each principal component. 
# The relationship (correlation or anticorrelation, etc) 
# between the initial variables and the principal components ($rotation). 
# The values of each sample in terms of the principal components ($x)
```

``` {r}
# We will now plot our pca.
# 

# 
library(usethis)

library(devtools)

library(ggbiplot)

ggbiplot(supermarket.pca)

```

## Conclusions
1. PCA1 to PCA4 are able to explain the variance in the data as they represent 99.96 of the variance. The other PC5 to 7 should be dropped.
2. The columns quantity, gross income,unit price and rating are able to explain the variance in the data,, so there are the most important features to be used in modelling


## Follow Up Questions
1. DId we have the right data
- We had the right data as we were able to perform PCA successfully
- MAybe to challenge our solution, maybe we couls try using the T_SNE algorithm
- Also dealing with outliers in the future would also increase the importance of our features






